{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Building the Train/Test/Val Splits\n",
    "This script normalizes the data and builds a tensorflow record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Eagerly? True\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "print(\"Executing Eagerly?\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_fh = 'rope_depth_3d_%s.tfrecords'\n",
    "depth_fh = '/scr2/varun/projs/data_intersection/%04d_depth.png'\n",
    "position_fh = '/scr2/varun/projs/data_intersection/%04d.txt'\n",
    "intersect_fh = '/scr2/varun/projs/data_intersection/%04d_intersect.txt'\n",
    "\n",
    "data_dir = '/scr2/varun/projs/data_intersection/'\n",
    "data_root = pathlib.Path(data_dir)\n",
    "\n",
    "size = 10000\n",
    "\n",
    "depth_pre = [None] * size\n",
    "position_pre = [None] * size\n",
    "intersect_pre = [None] * size\n",
    "\n",
    "for item in data_root.iterdir():\n",
    "    handle = str(item)\n",
    "    if \"mod\" not in handle:\n",
    "        idx = int(handle.split(\"/\")[-1][:4])\n",
    "        if \"depth\" in handle:\n",
    "            depth_pre[idx] = handle\n",
    "        elif \"intersect.txt\" in handle:\n",
    "            intersect_pre[idx] = handle\n",
    "        elif \".txt\" in handle:\n",
    "            position_pre[idx] = handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000.0 9500.0 10000.0\n",
      "['/scr2/varun/projs/data_intersection/7762_depth.png', '/scr2/varun/projs/data_intersection/7276_depth.png', '/scr2/varun/projs/data_intersection/7106_depth.png', '/scr2/varun/projs/data_intersection/7044_depth.png', '/scr2/varun/projs/data_intersection/8180_depth.png']\n",
      "['/scr2/varun/projs/data_intersection/7762.txt', '/scr2/varun/projs/data_intersection/7276.txt', '/scr2/varun/projs/data_intersection/7106.txt', '/scr2/varun/projs/data_intersection/7044.txt', '/scr2/varun/projs/data_intersection/8180.txt']\n",
      "['/scr2/varun/projs/data_intersection/7762_intersect.txt', '/scr2/varun/projs/data_intersection/7276_intersect.txt', '/scr2/varun/projs/data_intersection/7106_intersect.txt', '/scr2/varun/projs/data_intersection/7044_intersect.txt', '/scr2/varun/projs/data_intersection/8180_intersect.txt']\n"
     ]
    }
   ],
   "source": [
    "to_sample = list(range(size))\n",
    "\n",
    "random.seed(231)\n",
    "random.shuffle(to_sample)\n",
    "\n",
    "depth = [depth_pre[i] for i in to_sample]\n",
    "position = [position_pre[i] for i in to_sample]\n",
    "intersect = [intersect_pre[i] for i in to_sample]\n",
    "\n",
    "train_idx, test_idx, val_idx = [0.9 * size, 0.05 * size, 0.05 * size]\n",
    "val_idx += (train_idx + test_idx)\n",
    "test_idx += train_idx\n",
    "\n",
    "print(train_idx, test_idx, val_idx)\n",
    "print(depth[:5])\n",
    "print(position[:5])\n",
    "print(intersect[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "#     image = tf.image.resize(img_tensor, [224, 224])   # this would be if i wanted to resize\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess(path, path2, path3):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image), path2, path3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Just playing around with the data. Getting a feel for what the depth and numpy arrays are like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Value: 102\n",
      "Max Value: 110\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_num = 254\n",
    "\n",
    "img_raw = tf.io.read_file(depth[sample_num])\n",
    "img_tensor = tf.image.decode_png(img_raw, channels=3)\n",
    "# img_tensor = tf.image.resize(img_tensor, [224, 224])  # This would be if i wanted to resize\n",
    "\n",
    "print(\"Min Value:\", img_tensor.numpy().min())\n",
    "print(\"Max Value:\", img_tensor.numpy().max())\n",
    "\n",
    "img_tensor = tf.cast(img_tensor, tf.float32)\n",
    "img_tensor /= 255.0\n",
    "\n",
    "plt.imshow(img_tensor)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_raw = tf.io.read_file(position[sample_num])\n",
    "pos_raw = re.split(' |\\n', pos_raw.numpy().decode(\"utf-8\"))\n",
    "pos_raw = pos_raw[:-1]\n",
    "pos_raw = [float(pos) for pos in pos_raw]\n",
    "pos_tensor = tf.convert_to_tensor(pos_raw, tf.float32)\n",
    "pos_tensor = tf.reshape(pos_tensor, [int(len(pos_raw)/3), 3])\n",
    "pos_tensor = tf.transpose(pos_tensor[:, :2])       # choosing to get rid of the height.\n",
    "\n",
    "intersect_np = np.loadtxt(intersect[sample_num])\n",
    "intersect_tensor = tf.convert_to_tensor(intersect_np.flatten(), tf.float32)\n",
    "\n",
    "if pos_tensor[0, 0] > pos_tensor[0, -1]:\n",
    "    print(\"i flipped\")\n",
    "    pos_tensor = pos_tensor[:, ::-1]\n",
    "    intersect_tensor = intersect_tensor[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Dataset and Saving It\n",
    "\n",
    "Make sure to first compute the position dataset then the intersection dataset. We need to track if we flipped the position dataset. Any change should be reflected in the intersection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _intList_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _floatList_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def generate_example(depth_path, position_path, intersect_path):\n",
    "    depth_img = Image.open(depth_path)\n",
    "    depth_img = depth_img.resize((224, 224), resample=Image.LANCZOS)    # upscale to input into VGG\n",
    "    depth_img = np.array(depth_img)[:, :, :3]                           # removes alpha channel\n",
    "    \n",
    "    pos_arr = np.loadtxt(position_path)\n",
    "    pos_arr = pos_arr[:, :2].T.astype(np.float32)\n",
    "    \n",
    "    inter_arr = np.loadtxt(intersect_path)\n",
    "    inter_arr = inter_arr.flatten().astype(np.int64)\n",
    "    \n",
    "    has_inter = 1 if inter_arr[inter_arr != 0].size > 0 else 0\n",
    "    \n",
    "    if pos_arr[0, 0] > pos_arr[0, -1]:\n",
    "        pos_arr = pos_arr[:, ::-1]\n",
    "        inter_arr = inter_arr[::-1]\n",
    "        print(\"flipped for:\", position_path, intersect_path)\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'height' : _int64_feature(depth_img.shape[0]),\n",
    "        'width' : _int64_feature(depth_img.shape[1]),\n",
    "        'image_raw' : _bytes_feature(depth_img.tostring()),\n",
    "        'position' : _floatList_feature(pos_arr.flatten().tolist()),\n",
    "        'intersect' : _intList_feature(inter_arr.tolist()),\n",
    "        'has_intersection' : _int64_feature(has_inter)\n",
    "    }))\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record_file = 'assets/non-temporal/train-3d.tfrec'\n",
    "test_record_file = 'assets/non-temporal/test-3d.tfrec'\n",
    "val_record_file = 'assets/non-temporal/val-3d.tfrec'\n",
    "\n",
    "with tf.io.TFRecordWriter(train_record_file) as train_writer:\n",
    "    with tf.io.TFRecordWriter(test_record_file) as test_writer:\n",
    "        with tf.io.TFRecordWriter(val_record_file) as val_writer:\n",
    "            for ind, (dep, pos, inter) in enumerate(zip(depth, position, intersect)):\n",
    "                tf_example = generate_example(dep, pos, inter)\n",
    "                \n",
    "                if ind < train_idx:\n",
    "                    train_writer.write(tf_example.SerializeToString())\n",
    "                elif ind < test_idx:\n",
    "                    test_writer.write(tf_example.SerializeToString())\n",
    "                else:\n",
    "                    val_writer.write(tf_example.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
