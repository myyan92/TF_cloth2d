{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and Building the Dataset\n",
    "This script normalizes the data and builds a tensorflow record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing Eagerly? True\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "print(\"Executing Eagerly?\", tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrecords_fh = 'rope_depth_3d_%s.tfrecords'\n",
    "depth_fh = '/scr2/varun/projs/data_intersection/%04d_depth.png'\n",
    "position_fh = '/scr2/varun/projs/data_intersection/%04d.txt'\n",
    "intersect_fh = '/scr2/varun/projs/data_intersection/%04d_intersect.txt'\n",
    "\n",
    "data_dir = '/scr2/varun/projs/data_intersection/'\n",
    "data_root = pathlib.Path(data_dir)\n",
    "\n",
    "size = 10000\n",
    "\n",
    "depth = [None] * size\n",
    "position = [None] * size\n",
    "intersect = [None] * size\n",
    "\n",
    "for item in data_root.iterdir():\n",
    "    handle = str(item)\n",
    "    if \"mod\" not in handle:\n",
    "        idx = int(handle.split(\"/\")[-1][:4])\n",
    "        if \"depth\" in handle:\n",
    "            depth[idx] = handle\n",
    "        elif \"intersect.txt\" in handle:\n",
    "            intersect[idx] = handle\n",
    "        elif \".txt\" in handle:\n",
    "            position[idx] = handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "#     image = tf.image.resize(img_tensor, [224, 224])   # this would be if i wanted to resize\n",
    "    image /= 255.0\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess(path, path2, path3):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image), path2, path3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "Just playing around with the data. Getting a feel for what the depth and numpy arrays are like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Value: 102\n",
      "Max Value: 110\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_num = 254\n",
    "\n",
    "img_raw = tf.io.read_file(depth[sample_num])\n",
    "img_tensor = tf.image.decode_png(img_raw, channels=3)\n",
    "# img_tensor = tf.image.resize(img_tensor, [224, 224])  # This would be if i wanted to resize\n",
    "\n",
    "print(\"Min Value:\", img_tensor.numpy().min())\n",
    "print(\"Max Value:\", img_tensor.numpy().max())\n",
    "\n",
    "img_tensor = tf.cast(img_tensor, tf.float32)\n",
    "img_tensor /= 255.0\n",
    "\n",
    "plt.imshow(img_tensor)\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_raw = tf.io.read_file(position[sample_num])\n",
    "pos_raw = re.split(' |\\n', pos_raw.numpy().decode(\"utf-8\"))\n",
    "pos_raw = pos_raw[:-1]\n",
    "pos_raw = [float(pos) for pos in pos_raw]\n",
    "pos_tensor = tf.convert_to_tensor(pos_raw, tf.float32)\n",
    "pos_tensor = tf.reshape(pos_tensor, [int(len(pos_raw)/3), 3])\n",
    "pos_tensor = tf.transpose(pos_tensor[:, :2])       # choosing to get rid of the height.\n",
    "\n",
    "intersect_np = np.loadtxt(intersect[sample_num])\n",
    "intersect_tensor = tf.convert_to_tensor(intersect_np.flatten(), tf.float32)\n",
    "\n",
    "if pos_tensor[0, 0] > pos_tensor[0, -1]:\n",
    "    print(\"i flipped\")\n",
    "    pos_tensor = pos_tensor[:, ::-1]\n",
    "    intersect_tensor = intersect_tensor[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Dataset Object\n",
    "\n",
    "Make sure to first compute the position dataset then the intersection dataset. We need to track if we flipped the position dataset. Any change should be reflected in the intersection dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _intList_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "def _floatList_feature(value):\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "def generate_example(depth_path, position_path, intersect_path):\n",
    "    depth_img = Image.open(depth_path)\n",
    "    depth_img = depth_img.resize((224, 224), resample=Image.LANCZOS)    # upscale to input into VGG\n",
    "    depth_img = np.array(depth_img)[:, :, :3]                           # removes alpha channel\n",
    "    \n",
    "    pos_arr = np.loadtxt(position_path)\n",
    "    pos_arr = pos_arr[:, :2].T.astype(np.float32)\n",
    "    \n",
    "    inter_arr = np.loadtxt(intersect_path)\n",
    "    inter_arr = inter_arr.flatten().astype(np.int64)\n",
    "    \n",
    "    if pos_arr[0, 0] > pos_arr[0, -1]:\n",
    "        pos_arr = pos_arr[:, ::-1]\n",
    "        inter_arr = inter_arr[::-1]\n",
    "        print(\"flipped for:\", position_path, intersect_path)\n",
    "    \n",
    "    record = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'height' : _int64_feature(depth_img.shape[0]),\n",
    "        'width' : _int64_feature(depth_img.shape[1]),\n",
    "        'image_raw' : _bytes_feature(depth_img.tostring()),\n",
    "        'position' : _floatList_feature(pos_arr.flatten().tolist()),\n",
    "        'intersect' : _intList_feature(inter_arr.tolist())\n",
    "    }))\n",
    "    \n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_file = 'assets/non-temporal-3d.tfrec'\n",
    "\n",
    "with tf.io.TFRecordWriter(record_file) as writer:\n",
    "    for dep, pos, inter in zip(depth, position, intersect):\n",
    "        tf_example = generate_example(dep, pos, inter)\n",
    "        writer.write(tf_example.SerializeToString())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
